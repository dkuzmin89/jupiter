{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Importing required libraries\n",
    "'''\n",
    "this libraries required for current application\n",
    "all these libraries contained in basic python installation except\n",
    "for the pandas. pandas needed to be install via 'pip install pandas'\n",
    "'''\n",
    "\n",
    "import pandas as pd ## pandas library for working with tabular data\n",
    "import numpy as np  ## numpy librari for working with arrays\n",
    "import datetime     ## datetime library for working with dates\n",
    "import warnings     ## warnings library, to manage warnings\n",
    "warnings.filterwarnings('ignore') ## disable notofocations and warnings (sinse we are experimenting here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import section\n",
    "\n",
    "'''\n",
    "First of all we need to load input raw datasets.\n",
    "'''\n",
    "\n",
    "### JHU raw data imports\n",
    "inp_jhu_c = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv')\n",
    "inp_jhu_d = pd.read_csv('https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv')\n",
    "\n",
    "### WHO raw data input\n",
    "inp_who = pd.read_csv('/Users/dmitry/python_projects/who_gavi/inp/WHO-COVID-19-global-data-7.csv')\n",
    "\n",
    "### Coutry grouping file with 'gavi' columns\n",
    "cont_gr = pd.read_csv('inp/Country Groupings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Inspecting raw input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp_jhu_c.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inp_who.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cont_gr.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Searching in raw datasets\n",
    "\n",
    "# inp_jhu_c.loc[inp_jhu_c['Country/Region']=='China']\n",
    "# inp_who.loc[inp_who['Country Name']=='China'].tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nThe goal is to build dataset which will contain cases/deathes from both jhu and who raw data, moreover we need to add 'gavi' columns to output dataset\\nAs we can see raw datasets has different structure. We need to reshape jhu raw dataset, and prepare both raw dataset for merging. \\nWe are planning to merge data by country key and date key. Let's inspect countries from raw datasets\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "The goal is to build dataset which will contain cases/deathes from both jhu and who raw data, moreover we need to add 'gavi' columns to output dataset\n",
    "As we can see raw datasets has different structure. We need to reshape jhu raw dataset, and prepare both raw dataset for merging. \n",
    "We are planning to merge data by country key and date key. Let's inspect countries from raw datasets\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions \n",
    "\n",
    "'''\n",
    "this function compares two lists and return values that are not matching in both lists\n",
    "'''\n",
    "def returnNotMatches(a, b):\n",
    "    return [[x for x in a if x not in b], [x for x in b if x not in a]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generating country lists from raw data for inspection.\n",
    "\n",
    "jhu_country_list = inp_jhu_c['Country/Region'].unique().tolist() \n",
    "jhu_country_list.sort()\n",
    "who_country_list = inp_who['Country Name'].unique().tolist() \n",
    "who_country_list.sort()\n",
    "gavi_country_list = cont_gr['country'].unique().tolist() \n",
    "gavi_country_list.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "187"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# jhu_country_list\n",
    "len(jhu_country_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# who_country_list\n",
    "len(who_country_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "195"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# gavi_country_list\n",
    "len(gavi_country_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Comparing country names\n",
    "\n",
    "# returnNotMatches(gavi_country_list, jhu_country_list)\n",
    "# returnNotMatches(gavi_country_list, who_country_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nwe will take gavi coutry list as a reference. that means thas we have to rename countries in both jhu and who datasets.\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "we will take gavi coutry list as a reference. that means thas we have to rename countries in both jhu and who datasets.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries_to_remove = ['Czechia', 'Holy See', 'Liechtenstein', 'Taiwan*', 'Kosovo','Kosovo[1]', 'MS Zaandam', 'Western Sahara', 'Diamond Princess']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "jhu_to_rename = {\"Tanzania\": \"Tanzania, United Republic of\", \"Congo (Brazzaville)\": \"Congo, Republic of\",\n",
    "                 \"Congo (Kinshasa)\": \"Congo, Democratic Republic of the\",\n",
    "                 \"Cote d'Ivoire\": \"Côte d'Ivoire\", \"Laos\": \"Lao People's Democratic Republic\",\n",
    "                 \"Burma\": \"Myanmar\", \"Vietnam\": \"Viet Nam\", \"Syria\": \"Syrian Arab Republic\",\n",
    "                 \"Eswatini\": \"Swaziland\", \"Libya\": \"Libyan Arab Jamahiriya\",\n",
    "                 \"Cabo Verde\": \"Cape Verde\", \"Brunei\": \"Brunei Darussalam\",\n",
    "                 \"West Bank and Gaza\": \"Palestinian Territory\",\n",
    "                 \"North Macedonia\": \"Macedonia, Republic of\", \"Korea, South\": \"Korea, Republic of\",\n",
    "                 \"Russia\": \"Russian Federation\",\n",
    "                 \"Iran\": \"Iran, Islamic Republic of\",\n",
    "                 \"United Kingdom\": \"United Kingdom of Great Britain & Northern Ireland\",\n",
    "                 \"US\": \"United States\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "who_to_rename = {\"United Republic of Tanzania\": \"Tanzania, United Republic of\", \"Congo\": \"Congo, Republic of\",\n",
    "                 \"Democratic Republic of the Congo\": \"Congo, Democratic Republic of the\",\n",
    "                 \"Côte d’Ivoire\": \"Côte d'Ivoire\", \"Laos\": \"Lao People's Democratic Republic\",\n",
    "                 \"Burma\": \"Myanmar\", \"Vietnam\": \"Viet Nam\", \"Syria\": \"Syrian Arab Republic\", \"Eswatini\": \"Swaziland\",\n",
    "                 \"Libya\": \"Libyan Arab Jamahiriya\",\n",
    "                 \"Cabo Verde\": \"Cape Verde\", \"Brunei\": \"Brunei Darussalam\",\n",
    "                 \"occupied Palestinian territory, including east Jerusalem\": \"Palestinian Territory\",\n",
    "                 \"North Macedonia\": \"Macedonia, Republic of\", \"Republic of Korea\": \"Korea, Republic of\",\n",
    "                 \"Russia\": \"Russian Federation\",\n",
    "                 \"Iran (Islamic Republic of)\": \"Iran, Islamic Republic of\",\n",
    "                 \"The United Kingdom\": \"United Kingdom of Great Britain & Northern Ireland\",\n",
    "                 \"United States of America\": \"United States\", \"Bolivia (Plurinational State of)\": \"Bolivia\",\n",
    "                 \"Republic of Moldova\": \"Moldova\",\n",
    "                 \"Venezuela (Bolivarian Republic of)\": \"Venezuela\", \"Sint Maarten\": \"St Matrin\", \"Réunion\": \"Reunion\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### JHU data preparation section\n",
    "'''\n",
    "JHU dataset needs to be transposed. Dates from column names should be trasformed into values to form column with dates.\n",
    "to extract dates from column names we need to do following:\n",
    "'''\n",
    "\n",
    "jhu_columns = inp_jhu_c.iloc[:,4:].columns\n",
    "\n",
    "'''\n",
    "JHU max date\n",
    "'''\n",
    "jhu_max_date = inp_jhu_c.columns[-1]\n",
    "x = datetime.datetime.strptime(jhu_max_date, \"%m/%d/%y\")\n",
    "jhu_max_date = x.strftime(\"%m/%d/%Y\")\n",
    "\n",
    "'''\n",
    "here we use indexing. It is tricky approach by pandas (similar approach used for lists managing in python). So [:,4:] means that \n",
    "we take all rows (:) and take column names from fourth one until the letest one (4:) and take only columns values from it (.columns)\n",
    "Thants how we got the walues from column names\n",
    "'''\n",
    "\n",
    "'''\n",
    "Remove and rename countries\n",
    "'''\n",
    "inp_jhu_c['Country/Region'].replace(jhu_to_rename, inplace=True)\n",
    "inp_jhu_c = inp_jhu_c[~inp_jhu_c['Country/Region'].isin(countries_to_remove)]\n",
    "inp_jhu_d['Country/Region'].replace(jhu_to_rename, inplace=True)\n",
    "inp_jhu_d = inp_jhu_d[~inp_jhu_d['Country/Region'].isin(countries_to_remove)]\n",
    "\n",
    "\n",
    "'''\n",
    "Sort dataset to extract data in right order\n",
    "'''\n",
    "inp_jhu_c = inp_jhu_c.sort_values(['Country/Region','Province/State'])\n",
    "inp_jhu_d = inp_jhu_d.sort_values(['Country/Region','Province/State'])\n",
    "\n",
    "'''\n",
    "Here we are taking only values from all rows and from 4th column untill the last one. \n",
    "We will need this walues to form reshaped jhu dataset\n",
    "'''\n",
    "values_c = inp_jhu_c.iloc[:,4:].values.tolist()\n",
    "values_d = inp_jhu_d.iloc[:,4:].values.tolist()\n",
    "\n",
    "'''\n",
    "We've got a list of lists. Each row of values represent one element of the list \n",
    "[ [1,2,3,4...],[1,2,3,4...],[1,2,3,4...] ]\n",
    "we need to make it flatten (to make one big list of values), we are using nested loop for that:\n",
    "'''\n",
    "\n",
    "flat_list_c = []\n",
    "for sublist in values_c:\n",
    "    for item in sublist:\n",
    "        flat_list_c.append(item)\n",
    "flat_list_d = []\n",
    "for sublist in values_d:\n",
    "    for item in sublist:\n",
    "        flat_list_d.append(item)\n",
    "        \n",
    "'''\n",
    "Creating the variaty list on countries/regions/province/states just in case we will need them.\n",
    "Some of them will be used, some of them no.\n",
    "'''\n",
    "### JHU_regions/countries etc lists\n",
    "jhu_regions_list_nn = inp_jhu_c['Province/State'].to_list()\n",
    "jhu_regions = inp_jhu_c[inp_jhu_c['Province/State'].notnull()]\n",
    "jhu_regions_list = jhu_regions['Province/State'].to_list()\n",
    "jhu_countries_list = inp_jhu_c['Country/Region'].unique().tolist() \n",
    "jhu_countries_list_2 = inp_jhu_c['Country/Region'].tolist() \n",
    "jhu_count_regions = jhu_regions_list+jhu_countries_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "### JHU reshaping process\n",
    "'''\n",
    "Creating df with countries and provinces as a part of out jhu reshaping process\n",
    "'''\n",
    "\n",
    "'''\n",
    "New reshaped jhu dataset: \n",
    "1 column - dates\n",
    "2 colunm - 'Province/State'\n",
    "3 column - 'Country/Region'\n",
    "'''\n",
    "df1 = pd.DataFrame({'date': jhu_columns})  ### this is build in pandas function for DataFrame(dataset) creation\n",
    "df2 = inp_jhu_c[['Province/State','Country/Region']]\n",
    "\n",
    "\n",
    "'''\n",
    "We need to get resulting dataset as a table of time perions form 01/22/20 untill now and each country should have this time period. \n",
    "To do that we need to use a trick: to add same zero key to each dataframe (df1 and df2) and to merge them in 'outer' way by that key\n",
    "'''\n",
    "df1['key'] = 0\n",
    "df2['key'] = 0\n",
    "jhu_resh = df1.merge(df2, how='outer')\n",
    "\n",
    "'''\n",
    "Newx step is adding 'cases' and 'deaths' to our new df\n",
    "'''\n",
    "\n",
    "jhu_resh = jhu_resh.sort_values(['Country/Region', 'Province/State']) #just in case (we did it earlier)\n",
    "\n",
    "'''\n",
    "new 'confirmed' column - values from flatten_c list\n",
    "new 'deaths' column - values from flatten_d list\n",
    "'''\n",
    "jhu_resh['confirmed'] = flat_list_c\n",
    "jhu_resh['deaths'] = flat_list_d\n",
    "jhu_resh['date'] = pd.to_datetime(jhu_resh['date']).dt.strftime('%m/%d/%Y')  # we need to make this dates real dates to have an ability to compare and work with them\n",
    "\n",
    "'''\n",
    "again taking country and province lists to check if everithing is good and we did not miss any cointry or province\n",
    "'''\n",
    "res_list = jhu_resh['Country/Region'].unique().tolist()\n",
    "res_list_r = jhu_resh['Province/State'].unique().tolist()\n",
    "jhu_resh_c_r_list = res_list+res_list_r\n",
    "\n",
    "res_only_regions = jhu_resh.dropna(subset=[\"Province/State\"])  # df contains ONLY countries\n",
    "res_only_countries = jhu_resh[jhu_resh['Province/State'].isna()] # df contains ONLY province"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jhu_resh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "JHU dataset preparation and reshaping is DONE\n",
    "now it is time to WHO dataset preparation\n",
    "'''\n",
    "\n",
    "### WHO dataprep\n",
    "who = inp_who[~inp_who['Country Name'].isin(countries_to_remove)] ### removing countries that we are not using\n",
    "who['Country Name'].replace(who_to_rename, inplace=True) ### rename countries to match gavi naming\n",
    "who = who[['day','Country Name','Cumulative Deaths','Cumulative Confirmed']] ### keep only necessary columns\n",
    "who['day'] = pd.to_datetime(who.day).dt.strftime('%m/%d/%Y') # we need to make this dates real dates to have an ability to compare and work with them\n",
    "who = who[who.day >='01/22/2020'] ### keeping information only for time perion after 01/22/2020\n",
    "\n",
    "who_countries_list = who['Country Name'].unique().tolist() ### generationg country list to make sure that all countries are in place\n",
    "\n",
    "'''WHO max date'''\n",
    "who_max_date = who['day'].max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Next step is to deal with CHINA and AUSTRALIA. In JHU dataset they are not tracking them as a whole countries. They splitted them by province/states. \n",
    "So we need to aggregate\n",
    "'''\n",
    "\n",
    "######## CHINA\n",
    "\n",
    "### extraction CHINA from JHU reshaped df\n",
    "china = jhu_resh.loc[jhu_resh['Country/Region']=='China']\n",
    "china = china.groupby(['Country/Region','date'],as_index=False)['confirmed','deaths'].sum() ### summing 'confirmed' and 'deaths' by CHINA\n",
    "china['Province/State']=np.nan ### creating 'Province/State' and fill it up with NaN values\n",
    "\n",
    "### extraction CHINA from WHO df\n",
    "who_china = who.loc[who['Country Name']=='China']\n",
    "\n",
    "china_merged = pd.merge(china,who_china,left_on=['Country/Region','date'],right_on=['Country Name','day'],how='outer') # merging jhu china with who china (outer)\n",
    "china_merged = china_merged[['date','Province/State','Country/Region','confirmed','deaths','Cumulative Deaths','Cumulative Confirmed']] # keep only needed colomns\n",
    "china_merged = china_merged.rename(columns={'confirmed':'jhu_cases','deaths':'jhu_deaths','Cumulative Deaths':'WHO_deaths','Cumulative Confirmed':'WHO_cases'}) # rename columns\n",
    "'''\n",
    "we did 'outer' merge because we need to have values from all dates (from 01/22/20). In who dataset min date for countries is varies, so values that are missing \n",
    "in who was replased with NaN (same as missing values). In next step we are fillin nan values with zeroes and make this values integer.\n",
    "'''\n",
    "china_merged[['jhu_cases', 'jhu_deaths','WHO_deaths', 'WHO_cases']] = china_merged[['jhu_cases', 'jhu_deaths','WHO_deaths', 'WHO_cases']].fillna(0).astype(int) \n",
    "\n",
    "######## AUSTRALIA\n",
    "\n",
    "australia = jhu_resh.loc[jhu_resh['Country/Region']=='Australia']\n",
    "australia = australia.groupby(['Country/Region','date'],as_index=False)['confirmed','deaths'].sum()\n",
    "australia['Province/State']=np.nan\n",
    "\n",
    "who_australia = who.loc[who['Country Name']=='Australia']\n",
    "\n",
    "australia_merged = pd.merge(australia,who_australia,left_on=['Country/Region','date'],right_on=['Country Name','day'],how='outer')\n",
    "australia_merged = australia_merged[['date','Province/State','Country/Region','confirmed','deaths','Cumulative Deaths','Cumulative Confirmed']]\n",
    "australia_merged = australia_merged.rename(columns={'confirmed':'jhu_cases','deaths':'jhu_deaths','Cumulative Deaths':'WHO_deaths','Cumulative Confirmed':'WHO_cases'})\n",
    "australia_merged[['jhu_cases', 'jhu_deaths','WHO_deaths', 'WHO_cases']] = australia_merged[['jhu_cases', 'jhu_deaths','WHO_deaths', 'WHO_cases']].fillna(0).astype(int)\n",
    "\n",
    "### Append\n",
    "'''\n",
    "Just append China and Australia\n",
    "'''\n",
    "countries_to_add = china_merged.append(australia_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAIN MERGING\n",
    "\n",
    "\n",
    "merge_countries = pd.merge(res_only_countries,who,left_on=['date','Country/Region'],right_on=['day','Country Name'],how='outer')\n",
    "merge_countries[['Cumulative Deaths','Cumulative Confirmed']] = merge_countries[['Cumulative Deaths','Cumulative Confirmed']].fillna(0)\n",
    "merge_countries = merge_countries[['date','Province/State','Country/Region','confirmed','deaths','Cumulative Deaths','Cumulative Confirmed']]\n",
    "merge_countries = merge_countries.dropna(subset=['date']) ### removing side dates from outer join\n",
    "merge_countries = merge_countries.rename(columns={\"confirmed\":\"jhu_cases\",\"deaths\":\"jhu_deaths\",\"Cumulative Confirmed\":\"WHO_cases\",\"Cumulative Deaths\":\"WHO_deaths\"})\n",
    "\n",
    "\n",
    "merge_regions = pd.merge(res_only_regions,who,left_on=['date','Province/State'],right_on=['day','Country Name'],how='outer')\n",
    "merge_regions[['Cumulative Deaths','Cumulative Confirmed']] = merge_regions[['Cumulative Deaths','Cumulative Confirmed']].fillna(0)\n",
    "merge_regions = merge_regions[['date','Province/State','Country/Region','confirmed','deaths','Cumulative Deaths','Cumulative Confirmed']]\n",
    "merge_regions = merge_regions.dropna(subset=['date']) ### removing side dates from outer join\n",
    "merge_regions = merge_regions.rename(columns={\"confirmed\":\"jhu_cases\",\"deaths\":\"jhu_deaths\",\"Cumulative Confirmed\":\"WHO_cases\",\"Cumulative Deaths\":\"WHO_deaths\"})\n",
    "\n",
    "final_merge = merge_countries.append(merge_regions) ### append countries with provinces\n",
    "final_merge = final_merge.append(countries_to_add) ### append CHINA and AUSTRALIA\n",
    "\n",
    "'''\n",
    "Adding gavi columns. Sinse we've renamed all countries based on gavi country naming we just need to merge countryregion gavi's tadaset \n",
    "'''\n",
    "final_merge_f = pd.merge(final_merge,cont_gr,left_on='Country/Region',right_on='country')\n",
    "final_merge_f[['jhu_cases', 'jhu_deaths','WHO_deaths', 'WHO_cases']] = final_merge_f[['jhu_cases', 'jhu_deaths','WHO_deaths', 'WHO_cases']].astype(int) # this columns to integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_merge_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nSo basicly everithing is done. JHU and WHO data combined and gavi's columns added. Next step will be related to improving requests\\n\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "So basicly everithing is done. JHU and WHO data combined and gavi's columns added. Next step will be related to improving requests\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Additional columns\n",
    "\n",
    "'''\n",
    "Here we are adding additional columns to out dataset. One column with % gtowth and one column with numerical difference, both for 'cases' and 'deaths'\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    ".pct_change() - returns % changing for the specific column\n",
    ".round(4) - round values to 4 digits after the ,\n",
    ".replace([np.inf, -np.inf], np.nan) - when .pct_change() calculates diffetence beetween 0 and positive number it returns infinity or -infinity.\n",
    "                                    this function replase infinities with nan\n",
    ".fillna(0) - replacing nan with 0\n",
    ".clip(lower=0) - assigning lower boundary for the values to avoid negative values. (tricky part. actually % change can be negative as well as difference. but we have cases\n",
    "                where we have big number for 08/05/20 and 0 for 22/01/20. In that case % change wll show negative growth. We want to avoid this. Since \n",
    "                we are dealing with cumulative ceses next values could be only bigger or equal to prevoius ones.) # in latest version replaced by mask approach\n",
    ".diff() - returns numerical difference for specific column\n",
    "'''\n",
    "### For jhu_cases\n",
    "final_merge_f['growth_jhu_cases'] = final_merge_f['jhu_cases'].pct_change().round(4).replace([np.inf, -np.inf], np.nan).fillna(0)#.clip(lower=0)\n",
    "final_merge_f['diff_jhu_cases'] = final_merge_f['jhu_cases'].diff().fillna(0).astype(int)#.clip(lower=0)\n",
    "\n",
    "### For jhu_deaths\n",
    "final_merge_f['growth_jhu_deaths'] = final_merge_f['jhu_deaths'].pct_change().round(4).replace([np.inf, -np.inf], np.nan).fillna(0)#.clip(lower=0)\n",
    "final_merge_f['diff_jhu_deaths'] = final_merge_f['jhu_deaths'].diff().fillna(0).astype(int)#.clip(lower=0)\n",
    "\n",
    "### For who cases\n",
    "final_merge_f['growth_who_cases'] = final_merge_f['WHO_cases'].pct_change().round(4).replace([np.inf, -np.inf], np.nan).fillna(0)#.clip(lower=0)\n",
    "final_merge_f['diff_who_cases'] = final_merge_f['WHO_cases'].diff().fillna(0).astype(int)#.clip(lower=0)\n",
    "\n",
    "### For who deaths\n",
    "final_merge_f['growth_who_deaths'] = final_merge_f['WHO_deaths'].pct_change().round(4).replace([np.inf, -np.inf], np.nan).fillna(0)#.clip(lower=0)\n",
    "final_merge_f['diff_who_deaths'] = final_merge_f['WHO_deaths'].diff().fillna(0).astype(int)#.clip(lower=0)\n",
    "\n",
    "\n",
    "'''\n",
    "Allows keep negative values where they really can be. Insted of .clip()\n",
    "'''\n",
    "additional_cols = ['growth_jhu_cases','diff_jhu_cases','growth_jhu_deaths',\n",
    "                   'diff_jhu_deaths','growth_who_cases','diff_who_cases',\n",
    "                   'growth_who_deaths','diff_who_deaths']\n",
    "mask = (final_merge_f['date'] == final_merge_f['date'].min())\n",
    "final_merge_f[additional_cols] = final_merge_f[additional_cols].where(~mask, other=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Defining order of columns\n",
    "'''\n",
    "\n",
    "final_merge_f = final_merge_f[['date', 'Province/State', 'Country/Region',\n",
    "\n",
    "                 'jhu_cases', 'growth_jhu_cases', 'diff_jhu_cases', 'jhu_deaths', 'growth_jhu_deaths','diff_jhu_deaths',\n",
    "                 'WHO_cases', 'growth_who_cases', 'diff_who_cases', 'WHO_deaths', 'growth_who_deaths','diff_who_deaths',\n",
    "\n",
    "                 'Finance Country', 'cofinance_2017', 'cofinance_2018', 'cofinance_2019', 'continental_africa','country',\n",
    "                 'dov96', 'fragility_2017', 'fragility_2018', 'fragility_2019', 'francophone', 'gavi55', 'gavi68','gavi72',\n",
    "                 'gavi73', 'gavi77', 'gavi_region', 'gavi_region_sf', 'gavi_region_short', 'global', 'indo_pacific','iso3',\n",
    "                 'iso3_num', 'lang', 'pef_type', 'regional_je', 'regional_mena', 'regional_yfv', 'wb_long_2017','wb_long_2018',\n",
    "                 'wb_short_2017', 'wb_short_2018', 'who_region']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Final request was to rename columns, to remove rows for the provinces/states, to remove column provinces/states and to make PEF Tier column integer\n",
    "'''\n",
    "\n",
    "final_merge_f_f = final_merge_f.rename(columns={'Country/region': 'Country',\n",
    "                     'jhu_cases': 'JHU Cases', 'growth_jhu_cases': 'JHU Cases Growth',\n",
    "                     'diff_jhu_cases': 'JHU Cases Difference',\n",
    "                     'jhu_deaths': 'JHU Deaths', 'growth_jhu_deaths': 'JHU Deaths Growth',\n",
    "                     'diff_jhu_deaths': 'JHU Deaths Difference',\n",
    "                     'WHO_cases': 'WHO Cases', 'growth_who_cases': 'WHO Cases Growth',\n",
    "                     'diff_who_cases': 'WHO Cases Difference',\n",
    "                     'WHO_deaths': 'WHO Deaths', 'growth_who_deaths': 'WHO Deaths Growth',\n",
    "                     'diff_who_deaths': 'WHO Deaths Difference',\n",
    "                     'cofinance_2019': 'Cofinance 2019', 'fragility_2017': '2017 Fragility Status',\n",
    "                     'fragility_2018': '2018 Fragility Status',\n",
    "                     'fragility_2019': '2019 Fragility Status', 'gavi_region': 'Gavi Region', 'pef_type': 'PEF Tier',\n",
    "                     'who_region': 'WHO Region'})\n",
    "final_merge_f_f = final_merge_f_f[~final_merge_f_f['Province/State'].notna()]  # removing rows where province/stare NOT nan\n",
    "final_merge_f_f = final_merge_f_f.drop(['Province/State', 'country', 'gavi_region_sf'], axis=1)\n",
    "final_merge_f_f['PEF Tier'] = final_merge_f_f['PEF Tier'].replace({\"Not PEF\": 0}).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Saving output to desired format:\n",
    "'''\n",
    "dates_ok = True #jhu_max_date == who_max_date\n",
    "\n",
    "if not dates_ok:\n",
    "    print('WHO and JHU data sets have different last dates.')\n",
    "    print('WHO max date is {} , and JHU max date is {}'.format(who_max_date, jhu_max_date))\n",
    "    print('Please insure that you are using the WHO dataset with {} as latest date'.format(jhu_max_date))\n",
    "else:\n",
    "    final_merge_f_f.to_excel('DEMO/out/out_02.xlsx',index=False)\n",
    "    final_merge_f_f.to_csv('DEMO/out/out_02.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dmitry/python_projects/pandas/WHO and GAVI/DEMO'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
